{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Milestone 4: Privacy-Preserving Split Inference Demo\n\nThis notebook demonstrates the core building blocks of ZK-LLM-Turbo's split inference:\n1. CKKS public context (prove server can't decrypt)\n2. Homomorphic matrix multiplication\n3. Non-linear operations (RMSNorm, SiLU) comparison vs PyTorch\n4. Full 1-layer encrypted inference timing breakdown\n5. Accuracy comparison: encrypted vs plaintext\n6. KV caching: cached vs uncached attention\n7. Batching: reduced HTTP request count analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import tenseal as ts\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CKKS Public Context Demo\n",
    "\n",
    "The client creates a CKKS context with a secret key, then serializes it *without* the secret key.\n",
    "The server receives the public context and can perform computations but **cannot decrypt**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from client.encryption.ckks_context import create_ckks_context, serialize_public_context\n\n# Client creates full context (has secret key)\nclient_ctx = create_ckks_context(config_path=\"../client/config/client_config.yaml\")\n\n# Serialize public-only context\npublic_bytes = serialize_public_context(client_ctx)\nprint(f\"Public context size: {len(public_bytes) / 1024:.1f} KB\")\n\n# Server loads public context\nserver_ctx = ts.context_from(public_bytes)\n\n# Client encrypts\nsecret_data = [3.14, 2.71, 1.41, 1.73]\nenc_vec = ts.ckks_vector(client_ctx, secret_data)\n\n# Server receives ciphertext\nserver_vec = ts.ckks_vector_from(server_ctx, enc_vec.serialize())\n\n# Server CANNOT decrypt\ntry:\n    server_vec.decrypt()\n    print(\"ERROR: Server decrypted! This should not happen.\")\nexcept Exception as e:\n    print(f\"Server cannot decrypt (expected): {type(e).__name__}\")\n\n# Client CAN decrypt\nclient_result = ts.ckks_vector_from(client_ctx, enc_vec.serialize())\nprint(f\"Client decrypts: {client_result.decrypt()[:4]}\")\nprint(f\"Original data:   {secret_data}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HE Matrix Multiplication Demo\n",
    "\n",
    "Server computes `Enc(x) @ W` homomorphically without seeing `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a small projection: 64-dim input → 32-dim output\n",
    "dim_in, dim_out = 64, 32\n",
    "x = np.random.randn(dim_in).astype(np.float32) * 0.1\n",
    "W = np.random.randn(dim_in, dim_out).astype(np.float32) * 0.1\n",
    "\n",
    "# Plaintext result\n",
    "expected = x @ W\n",
    "\n",
    "# Client encrypts x\n",
    "enc_x = ts.ckks_vector(client_ctx, x.tolist())\n",
    "\n",
    "# Server computes enc_x @ W\n",
    "server_enc = ts.ckks_vector_from(server_ctx, enc_x.serialize())\n",
    "t0 = time.perf_counter()\n",
    "enc_result = server_enc.mm(W.tolist())\n",
    "he_time = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "# Client decrypts\n",
    "actual = np.array(ts.ckks_vector_from(client_ctx, enc_result.serialize()).decrypt()[:dim_out])\n",
    "\n",
    "error = np.abs(actual - expected)\n",
    "print(f\"HE matmul ({dim_in}→{dim_out}): {he_time:.1f} ms\")\n",
    "print(f\"Max error: {error.max():.6f}\")\n",
    "print(f\"Mean error: {error.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-scale: 2048-dim input → 256-dim output (like K/V projection)\n",
    "dim_in, dim_out = 2048, 256\n",
    "x = np.random.randn(dim_in).astype(np.float32) * 0.01\n",
    "W = np.random.randn(dim_in, dim_out).astype(np.float32) * 0.01\n",
    "\n",
    "expected = x @ W\n",
    "enc_x = ts.ckks_vector(client_ctx, x.tolist())\n",
    "server_enc = ts.ckks_vector_from(server_ctx, enc_x.serialize())\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "enc_result = server_enc.mm(W.tolist())\n",
    "he_time = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "actual = np.array(ts.ckks_vector_from(client_ctx, enc_result.serialize()).decrypt()[:dim_out])\n",
    "error = np.abs(actual - expected)\n",
    "\n",
    "print(f\"HE matmul ({dim_in}→{dim_out}): {he_time:.1f} ms\")\n",
    "print(f\"Max error: {error.max():.6f}\")\n",
    "print(f\"Mean error: {error.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Non-Linear Operations: Our NumPy vs PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from client.inference.nonlinear_ops import rms_norm, silu, softmax\n",
    "\n",
    "# RMSNorm comparison\n",
    "x_np = np.random.randn(2048).astype(np.float32)\n",
    "w_np = np.random.randn(2048).astype(np.float32)\n",
    "\n",
    "our_norm = rms_norm(x_np, w_np, eps=1e-5)\n",
    "\n",
    "x_t = torch.tensor(x_np)\n",
    "w_t = torch.tensor(w_np)\n",
    "variance = x_t.pow(2).mean(-1, keepdim=True)\n",
    "pt_norm = ((x_t * torch.rsqrt(variance + 1e-5)) * w_t).numpy()\n",
    "\n",
    "print(f\"RMSNorm max diff: {np.abs(our_norm - pt_norm).max():.2e}\")\n",
    "\n",
    "# SiLU comparison\n",
    "x_np = np.random.randn(1000).astype(np.float32)\n",
    "our_silu = silu(x_np)\n",
    "pt_silu = torch.nn.functional.silu(torch.tensor(x_np)).numpy()\n",
    "print(f\"SiLU max diff: {np.abs(our_silu - pt_silu).max():.2e}\")\n",
    "\n",
    "# Softmax comparison\n",
    "x_np = np.random.randn(32, 32).astype(np.float32)\n",
    "our_sm = softmax(x_np)\n",
    "pt_sm = torch.nn.functional.softmax(torch.tensor(x_np), dim=-1).numpy()\n",
    "print(f\"Softmax max diff: {np.abs(our_sm - pt_sm).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Timing Breakdown: Encrypted Operations\n",
    "\n",
    "Measure the cost of each component in a single round-trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 2048\n",
    "kv_dim = 256\n",
    "ffn_dim = 5632\n",
    "\n",
    "x = np.random.randn(hidden_dim).astype(np.float32) * 0.01\n",
    "W_q = np.random.randn(hidden_dim, hidden_dim).astype(np.float32) * 0.005\n",
    "W_k = np.random.randn(hidden_dim, kv_dim).astype(np.float32) * 0.005\n",
    "\n",
    "# Encryption time\n",
    "t0 = time.perf_counter()\n",
    "enc_x = ts.ckks_vector(client_ctx, x.tolist())\n",
    "encrypt_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "# Serialize time\n",
    "t0 = time.perf_counter()\n",
    "enc_bytes = enc_x.serialize()\n",
    "serialize_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "# Matmul Q (2048→2048)\n",
    "t0 = time.perf_counter()\n",
    "enc_q = enc_x.mm(W_q.tolist())\n",
    "mm_q_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "# Matmul K (2048→256)\n",
    "enc_x2 = ts.ckks_vector(client_ctx, x.tolist())  # fresh for fair comparison\n",
    "t0 = time.perf_counter()\n",
    "enc_k = enc_x2.mm(W_k.tolist())\n",
    "mm_k_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "# Decrypt time\n",
    "t0 = time.perf_counter()\n",
    "dec = enc_q.decrypt()\n",
    "decrypt_ms = (time.perf_counter() - t0) * 1000\n",
    "\n",
    "print(\"=== Timing Breakdown (single token) ===\")\n",
    "print(f\"Encrypt (2048-dim):     {encrypt_ms:8.1f} ms\")\n",
    "print(f\"Serialize:              {serialize_ms:8.1f} ms\")\n",
    "print(f\"HE matmul 2048→2048:    {mm_q_ms:8.1f} ms\")\n",
    "print(f\"HE matmul 2048→256:     {mm_k_ms:8.1f} ms\")\n",
    "print(f\"Decrypt (2048-dim):     {decrypt_ms:8.1f} ms\")\n",
    "print(f\"Ciphertext size:        {len(enc_bytes) / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Accuracy: Encrypted vs Plaintext Layer Output\n",
    "\n",
    "Compare a full encrypted matmul chain (simulating Q projection) against plaintext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate: RMSNorm → encrypt → Q projection → decrypt\n",
    "hidden_dim = 2048\n",
    "x = np.random.randn(hidden_dim).astype(np.float32) * 0.01\n",
    "norm_w = np.ones(hidden_dim, dtype=np.float32)\n",
    "W_q = np.random.randn(hidden_dim, hidden_dim).astype(np.float32) * 0.005\n",
    "\n",
    "# Plaintext pipeline\n",
    "x_normed = rms_norm(x, norm_w, 1e-5)\n",
    "q_plain = x_normed @ W_q\n",
    "\n",
    "# Encrypted pipeline\n",
    "enc_normed = ts.ckks_vector(client_ctx, x_normed.tolist())\n",
    "enc_q = enc_normed.mm(W_q.tolist())\n",
    "q_enc = np.array(enc_q.decrypt()[:hidden_dim], dtype=np.float32)\n",
    "\n",
    "error = np.abs(q_plain - q_enc)\n",
    "print(f\"Encrypted vs Plaintext Q-projection ({hidden_dim}→{hidden_dim}):\")\n",
    "print(f\"  Max error:  {error.max():.6f}\")\n",
    "print(f\"  Mean error: {error.mean():.6f}\")\n",
    "print(f\"  Relative error: {(error / (np.abs(q_plain) + 1e-10)).mean():.4%}\")\n",
    "\n",
    "# Cosine similarity\n",
    "cos_sim = np.dot(q_plain, q_enc) / (np.linalg.norm(q_plain) * np.linalg.norm(q_enc))\n",
    "print(f\"  Cosine similarity: {cos_sim:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary (Sections 1-5)\n\n- **Privacy**: Server has public context only → cannot decrypt any ciphertexts\n- **Correctness**: HE matmul matches plaintext within CKKS tolerance (~0.01 for small dims)\n- **Non-linear ops**: Our NumPy implementations match PyTorch within float32 epsilon\n- **Performance**: Single HE matmul at 2048 dims takes ~X ms per token\n- **Architecture**: 4 round-trips per layer, configurable number of encrypted layers"
  },
  {
   "cell_type": "markdown",
   "source": "## 6. KV Caching: Cached vs Uncached Attention\n\nAfter the initial prompt, only the new token needs Q/K/V computation.\nCached K and V from previous tokens are reused for attention.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from client.inference.nonlinear_ops import (\n    compute_attention,\n    compute_attention_cached,\n    apply_rotary_embeddings,\n    apply_rotary_embeddings_at_positions,\n)\n\nnum_heads = 32\nnum_kv_heads = 4\nhead_dim = 64\nhidden_dim = num_heads * head_dim  # 2048\nkv_dim = num_kv_heads * head_dim   # 256\n\n# Simulate a 5-token prompt\nseq_len = 5\nq_all = np.random.randn(seq_len, hidden_dim).astype(np.float32) * 0.1\nk_all = np.random.randn(seq_len, kv_dim).astype(np.float32) * 0.1\nv_all = np.random.randn(seq_len, kv_dim).astype(np.float32) * 0.1\n\n# Apply RoPE to full sequence\npositions_full = np.arange(seq_len)\nq_rot, k_rot = apply_rotary_embeddings_at_positions(\n    q_all, k_all, positions_full,\n    head_dim=head_dim, num_heads=num_heads, num_kv_heads=num_kv_heads,\n)\n\n# --- Full attention (no cache) ---\nt0 = time.perf_counter()\nattn_full = compute_attention_cached(\n    q_rot, k_rot, v_all,\n    num_heads=num_heads, num_kv_heads=num_kv_heads, head_dim=head_dim,\n)\nfull_ms = (time.perf_counter() - t0) * 1000\n\n# --- Incremental attention (simulating cache) ---\n# Cache K, V from first 4 tokens, compute attention for token 5 only\nk_cache = k_rot[:4]\nv_cache = v_all[:4]\nq_new = q_rot[4:5]       # single new query\nk_new = k_rot[4:5]       # single new key\nv_new = v_all[4:5]       # single new value\n\n# Append to cache\nk_full = np.concatenate([k_cache, k_new], axis=0)\nv_full = np.concatenate([v_cache, v_new], axis=0)\n\nt0 = time.perf_counter()\nattn_cached = compute_attention_cached(\n    q_new, k_full, v_full,\n    num_heads=num_heads, num_kv_heads=num_kv_heads, head_dim=head_dim,\n)\ncached_ms = (time.perf_counter() - t0) * 1000\n\n# Compare: last token's output should match\ndiff = np.abs(attn_full[-1:] - attn_cached).max()\nprint(f\"Full attention ({seq_len} tokens):     {full_ms:.3f} ms\")\nprint(f\"Cached attention (1 new token):  {cached_ms:.3f} ms\")\nprint(f\"Max difference (last token):     {diff:.2e}\")\nprint(f\"\\nWith caching, only 1 token goes through HE rounds instead of {seq_len}.\")\nprint(f\"For a 6-token prompt generating 5 tokens:\")\nprint(f\"  Without cache: 6+7+8+9+10 = 40 tokens through all layers\")\nprint(f\"  With cache:    6+1+1+1+1  = 10 tokens through all layers (4x reduction)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Batching: HTTP Request Count Analysis\n\nWith batched requests, all tokens in a step are sent in a single HTTP request per round.\nThis table shows the reduction in total HTTP requests for a typical generation run.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "prompt_len = 6\nnum_generated = 5\nrounds_per_layer = 4\nnum_encrypted_layers = 1\n\nprint(\"=== HTTP Request Count: Per-Token (old) vs Batched + KV Cache (new) ===\\n\")\nprint(f\"Prompt length: {prompt_len} tokens, generating {num_generated} tokens\\n\")\n\n# Old: per-token, no cache\nold_total = 0\nprint(f\"{'Step':<8} {'Tokens':<10} {'Old (per-token)':<20} {'New (batched+cache)':<20}\")\nprint(\"-\" * 58)\nfor step in range(num_generated):\n    seq_len_old = prompt_len + step\n    old_reqs = seq_len_old * rounds_per_layer * num_encrypted_layers\n    old_total += old_reqs\n\n    if step == 0:\n        new_reqs = rounds_per_layer * num_encrypted_layers  # batch all prompt tokens\n        new_tokens = prompt_len\n    else:\n        new_reqs = rounds_per_layer * num_encrypted_layers  # only 1 new token\n        new_tokens = 1\n\n    print(f\"{step:<8} {new_tokens:<10} {old_reqs:<20} {new_reqs:<20}\")\n\nnew_total = rounds_per_layer * num_encrypted_layers * num_generated\n\nprint(\"-\" * 58)\nprint(f\"{'Total':<8} {'':<10} {old_total:<20} {new_total:<20}\")\nprint(f\"\\nReduction: {old_total} → {new_total} HTTP requests ({old_total / new_total:.0f}x fewer)\")\n\n# HE matmuls comparison\nhe_per_token = 10  # approximate: 3(QKV) + 1(O) + 4(gate+up split) + 2(down split)\nold_he = sum((prompt_len + s) * he_per_token for s in range(num_generated))\nnew_he = prompt_len * he_per_token + (num_generated - 1) * he_per_token\nprint(f\"\\nHE matmuls: {old_he} → {new_he} ({old_he / new_he:.1f}x fewer)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Full Summary\n\n| Feature | Details |\n|---------|---------|\n| **Privacy** | Server has public context only — cannot decrypt any ciphertexts |\n| **Correctness** | HE matmul matches plaintext within CKKS tolerance (~0.01 for small dims) |\n| **Non-linear ops** | NumPy implementations match PyTorch within float32 epsilon |\n| **KV Caching** | Cached attention matches full attention exactly; 4x fewer tokens processed |\n| **Batching** | 8x fewer HTTP requests for typical generation runs |\n| **CLI** | `--prompt`, `--num-tokens`, `--num-encrypted-layers`, `--logs`, `--stats` |\n| **Architecture** | 4 batched round-trips per layer, configurable encrypted layers |",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}