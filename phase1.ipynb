{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27c4095",
   "metadata": {},
   "source": [
    "# Phase 1: Planning & Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae1358",
   "metadata": {},
   "source": [
    "Make sure all requirements are installed before running any code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d833baf",
   "metadata": {},
   "source": [
    "### Step 1: Load & Benchmark Model in Plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0879358d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rojo/Documents/repos/ZK-LLM-Turbo/split-inference-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the benefits of homomorphic encryption.\n",
      "Latency: 0.58 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Explain the benefits of homomorphic encryption.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=32)\n",
    "end = time.time()\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(f\"Latency: {end - start:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90bea28",
   "metadata": {},
   "source": [
    "### Step 2: Identify Linear vs Non-Linear Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26965af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 0.58 seconds\n",
      " <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "model <class 'transformers.models.llama.modeling_llama.LlamaModel'>\n",
      "model.embed_tokens <class 'torch.nn.modules.sparse.Embedding'>\n",
      "model.layers <class 'torch.nn.modules.container.ModuleList'>\n",
      "model.layers.0 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.0.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.0.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.0.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.0.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.0.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.1 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.1.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.1.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.1.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.1.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.1.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.2 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.2.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.2.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.2.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.2.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.2.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.3 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.3.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.3.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.3.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.3.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.3.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.4 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.4.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.4.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.4.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.4.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.4.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.5 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.5.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.5.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.5.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.5.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.5.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.6 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.6.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.6.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.6.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.6.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.6.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.7 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.7.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.7.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.7.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.7.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.7.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.8 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.8.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.8.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.8.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.8.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.8.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.9 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.9.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.9.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.9.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.9.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.9.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.10 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.10.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.10.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.10.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.10.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.10.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.11 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.11.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.11.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.11.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.11.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.11.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.12 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.12.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.12.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.12.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.12.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.12.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.13 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.13.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.13.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.13.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.13.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.13.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.14 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.14.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.14.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.14.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.14.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.14.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.15 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.15.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.15.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.15.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.15.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.15.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.16 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.16.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.16.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.16.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.16.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.16.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.17 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.17.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.17.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.17.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.17.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.17.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.18 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.18.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.18.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.18.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.18.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.18.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.19 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.19.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.19.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.19.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.19.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.19.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.20 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.20.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.20.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.20.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.20.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.20.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.21 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.21.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.21.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.21.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.21.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.21.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.norm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.rotary_emb <class 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding'>\n",
      "lm_head <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Latency: {end - start:.2f} seconds\")\n",
    "for name, module in model.named_modules():\n",
    "    print(name, type(module))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d6a21",
   "metadata": {},
   "source": [
    "### Step 3: Collect Baseline Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29342190",
   "metadata": {},
   "source": [
    "Run a small evaluation to get perplexity (language-model quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a688d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 342401.78 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 1984312.40 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 1400460.26 examples/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2261 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline perplexity: 6.13\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import math\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.eval()\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:1%]\")  # small subset\n",
    "encodings = tokenizer(\"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "max_length = model.config.max_position_embeddings\n",
    "stride = 512\n",
    "nlls = []\n",
    "for i in range(0, encodings.input_ids.size(1), stride):\n",
    "    begin_loc = max(i + stride - max_length, 0)\n",
    "    end_loc = i + stride\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "    target_ids = input_ids.clone()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        neg_log_likelihood = outputs.loss\n",
    "    nlls.append(neg_log_likelihood)\n",
    "ppl = math.exp(torch.stack(nlls).mean())\n",
    "print(f\"Baseline perplexity: {ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcc0b26",
   "metadata": {},
   "source": [
    "### Step 4: Measure Memory Footprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f8f27",
   "metadata": {},
   "source": [
    "Use psutil or torch.cuda.memory_allocated() (if you had GPU).\n",
    "On macOS CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5928668e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used: 5953.2 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "process = psutil.Process()\n",
    "mem_MB = process.memory_info().rss / 1024 ** 2\n",
    "print(f\"Memory used: {mem_MB:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426033a",
   "metadata": {},
   "source": [
    "### Step 5: Log the Architecture (Linear vs Non-Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff9f0e",
   "metadata": {},
   "source": [
    "Run this once and save to model_layers.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "787a887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model_layers.txt\", \"w\") as f:\n",
    "    for name, module in model.named_modules():\n",
    "        f.write(f\"{name} : {type(module)}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "split-inference-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
