{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27c4095",
   "metadata": {},
   "source": [
    "# Phase 1: Planning & Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d833baf",
   "metadata": {},
   "source": [
    "### Step 1: Load & Benchmark Model in Plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0879358d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rojo/Documents/repos/ZK-LLM-Turbo/split-inference-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain the benefits of homomorphic encryption.\n",
      "Latency: 0.66 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "prompt = \"Explain the benefits of homomorphic encryption.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=32)\n",
    "end = time.time()\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(f\"Latency: {end - start:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90bea28",
   "metadata": {},
   "source": [
    "### Step 2: Identify Linear vs Non-Linear Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26965af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 0.66 seconds\n",
      " <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "model <class 'transformers.models.llama.modeling_llama.LlamaModel'>\n",
      "model.embed_tokens <class 'torch.nn.modules.sparse.Embedding'>\n",
      "model.layers <class 'torch.nn.modules.container.ModuleList'>\n",
      "model.layers.0 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.0.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.0.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.0.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.0.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.0.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.0.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.1 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.1.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.1.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.1.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.1.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.1.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.1.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.2 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.2.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.2.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.2.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.2.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.2.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.2.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.3 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.3.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.3.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.3.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.3.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.3.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.3.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.4 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.4.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.4.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.4.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.4.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.4.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.4.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.5 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.5.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.5.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.5.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.5.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.5.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.5.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.6 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.6.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.6.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.6.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.6.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.6.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.6.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.7 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.7.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.7.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.7.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.7.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.7.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.7.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.8 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.8.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.8.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.8.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.8.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.8.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.8.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.9 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.9.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.9.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.9.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.9.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.9.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.9.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.10 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.10.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.10.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.10.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.10.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.10.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.10.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.11 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.11.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.11.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.11.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.11.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.11.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.11.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.12 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.12.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.12.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.12.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.12.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.12.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.12.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.13 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.13.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.13.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.13.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.13.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.13.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.13.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.14 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.14.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.14.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.14.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.14.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.14.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.14.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.15 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.15.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.15.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.15.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.15.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.15.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.15.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.16 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.16.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.16.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.16.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.16.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.16.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.16.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.17 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.17.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.17.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.17.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.17.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.17.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.17.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.18 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.18.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.18.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.18.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.18.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.18.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.18.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.19 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.19.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.19.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.19.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.19.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.19.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.19.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.20 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.20.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.20.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.20.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.20.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.20.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.20.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.21 <class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "model.layers.21.self_attn <class 'transformers.models.llama.modeling_llama.LlamaAttention'>\n",
      "model.layers.21.self_attn.q_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.self_attn.k_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.self_attn.v_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.self_attn.o_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.mlp <class 'transformers.models.llama.modeling_llama.LlamaMLP'>\n",
      "model.layers.21.mlp.gate_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.mlp.up_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.mlp.down_proj <class 'torch.nn.modules.linear.Linear'>\n",
      "model.layers.21.mlp.act_fn <class 'torch.nn.modules.activation.SiLU'>\n",
      "model.layers.21.input_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.layers.21.post_attention_layernorm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.norm <class 'transformers.models.llama.modeling_llama.LlamaRMSNorm'>\n",
      "model.rotary_emb <class 'transformers.models.llama.modeling_llama.LlamaRotaryEmbedding'>\n",
      "lm_head <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Latency: {end - start:.2f} seconds\")\n",
    "for name, module in model.named_modules():\n",
    "    print(name, type(module))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "split-inference-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
